# -*- coding: utf-8 -*-
"""Logistic and SVM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1arxXvIR56EesFXFoH7eyZFM-AWLpg6DV

# Logistic and SVM Model
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import metrics
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import SGDClassifier
from sklearn.linear_model import Lasso
from sklearn.linear_model import Ridge
from sklearn.linear_model import ElasticNet
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.metrics import classification_report
from sklearn.metrics import roc_auc_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RandomizedSearchCV
import lightgbm as lgb
import xgboost as xgb
import sklearn.datasets
import gc
import warnings
warnings.filterwarnings('ignore')

train_tiny_X = pd.read_csv('music_data/train_tiny_X.csv')
train_tiny_y = pd.read_csv('music_data/train_tiny_Y.csv')

display(train_tiny_y)

train_X = pd.read_csv('music_data/train_X.csv')
val_X = pd.read_csv('music_data/valid_X.csv')
train_y = pd.read_csv('music_data/train_Y.csv')
val_y = pd.read_csv('music_data/valid_Y.csv')

val_tiny_X = val_X[:1000]
val_tiny_y = val_y[:1000]

catCols = ['msno', 'song_id', 'source_screen_name', 'source_system_tab', 'source_type', 'genre_ids', 'artist_name', 
           'composer', 'lyricist', 'language', 'city', 'gender', 'registered_via']
numCols = ['bd', 'song_length']

train_X.info()

print(np.sum(train_X.isnull()))
print(train_X.shape)

all_train = pd.concat([train_X, train_y])
all_train.shape

np.sum(train_y.isnull())

train_y.head(100)

all_train.dropna(axis=0, how='any').shape

"""## Logistic Regression"""

# numRow = len(all_data)
# def checkCol(df, colName, numRow, isCata = True):
#     print(colName, ':')
#     print('Any NA?: ', df[colName].isnull().values.any())
#     if df[colName].isnull().values.any():
#         numNA = sum(df[colName].isnull().values)
#         print ('    # of NA: ', numNA)
#         print ('    NA%:     ', numNA/numRow)
#     if isCata:
#         levelList = df[colName].unique()
#         print('Different levels: ', len(levelList), levelList)
#         fillna(df, colName, isCata = True)
        
#     else:
#         print('range: ', min(df[colName].astype(float).dropna()), max(df[colName].astype(float).dropna()))
#         fillna(df, colName, isCata = False)
        
# def fillna(df, colName, isCata = True):
#     if isCata:
#         df[colName] = df[colName].fillna('0')
#     else:
#         if df[colName].isnull().values.any():
#             df[colName+'_dm'] = df[colName].isnull().astype(int)
#         df[colName] = df[colName].fillna(0)
#     return df

#debugging with tiny data set
numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])
categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])
preprocessor = ColumnTransformer(transformers=[('num', numeric_transformer, numCols),
                                               ('cat', categorical_transformer, catCols)])
clf = Pipeline(steps=[('preprocessor', preprocessor),
                      ('classifier', LogisticRegression())])

clf.fit(train_tiny_X, train_tiny_y['target'])
print("Model score on training set:")
print("model score: %.3f" % clf.score(train_tiny_X, train_tiny_y['target']))
print(classification_report(train_tiny_y['target'], clf.predict(train_tiny_X)))
print(roc_auc_score(train_tiny_y['target'], clf.predict_proba(train_tiny_X)[:,1]))

print("Model score on validation set:")
print("model score: %.3f" % clf.score(val_tiny_X, val_tiny_y['target']))
print(classification_report(val_tiny_y['target'], clf.predict(val_tiny_X)))
print(roc_auc_score(val_tiny_y['target'], clf.predict_proba(val_tiny_X)[:,1]))

# DON'T RUN IT YET!!!!!! HUGE DATASET!!!

# numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])
# categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])
# preprocessor = ColumnTransformer(transformers=[('num', numeric_transformer, numCols),
#                                                ('cat', categorical_transformer, catCols)])
# clf = Pipeline(steps=[('preprocessor', preprocessor),
#                       ('classifier', LogisticRegression())])

# clf.fit(train_X, train_y['target'])
# print("model score: %.3f" % clf.score(val_X, val_y['target']))
# print(classification_report(val_y['target'], clf.predict(val_X)))
# print(roc_auc_score(val_y, clf.predict_proba(val_X)['target']))

"""Default: Logistic Regression with No Penalty, learning rate = 'optimal'  
val acc = 0.66
"""

#mini batch training with full data and early stopping
numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])
categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])
preprocessor = ColumnTransformer(transformers=[('num', numeric_transformer, numCols),
                                               ('cat', categorical_transformer, catCols)])
clf_log = Pipeline(steps=[('preprocessor', preprocessor),
                ('classifier', SGDClassifier(loss='log', penalty=None, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, 
                                max_iter=1000, tol=0.001, shuffle=True, verbose=1, epsilon=0.1, n_jobs=None, 
                                random_state=None, learning_rate='adaptive', eta0=0.0001, 
                                power_t=0.5, early_stopping=True, validation_fraction=0.1, n_iter_no_change=5, 
                                class_weight=None, warm_start=True, average=False))])
#default, no penalty
clf_log.fit(train_X, train_y['target'])
print("model score: %.3f" % clf_log.score(val_X, val_y['target']))
print(classification_report(val_y['target'], clf_log.predict(val_X)))
print(roc_auc_score(val_y['target'], clf_log.predict_proba(val_X)[:,1]))
#val acc 0.660

numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])
categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])
preprocessor = ColumnTransformer(transformers=[('num', numeric_transformer, numCols),
                                               ('cat', categorical_transformer, catCols)])
clf_svm = Pipeline(steps=[('preprocessor', preprocessor),
                ('classifier', SGDClassifier(loss='hinge', penalty=None, early_stopping=True, 
                                             validation_fraction=0.1, n_iter_no_change=5))])
#default, no penalty
clf_svm.fit(train_X, train_y['target'])
print("model score: %.3f" % clf_svm.score(val_X, val_y['target']))
print(classification_report(val_y['target'], clf_svm.predict(val_X)))
print(roc_auc_score(val_y['target'], clf_svm.predict(val_X)>0.5))

"""best SVM model(with no penalty):  
learning rate = 'adaptive', eta = 0.01  
best val acc = 0.697
"""

etas = [0.01]
best_model_svm = None
best_model_score_svm = 0.0
numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])
categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])
preprocessor = ColumnTransformer(transformers=[('num', numeric_transformer, numCols),
                                               ('cat', categorical_transformer, catCols)])
for eta in etas:
    clf_svm = Pipeline(steps=[('preprocessor', preprocessor),
                    ('classifier', SGDClassifier(loss='hinge', penalty=None, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, 
                                    max_iter=1000, tol=0.001, shuffle=True, verbose=0, epsilon=0.1, n_jobs=None, 
                                    random_state=None, learning_rate='adaptive', eta0=eta, 
                                    power_t=0.5, early_stopping=True, validation_fraction=0.1, n_iter_no_change=5, 
                                    class_weight=None, warm_start=True, average=False))])
    clf_svm.fit(train_X, train_y['target'])
    model_score = clf_svm.score(val_X, val_y['target'])
    if model_score > best_model_score_svm:
        best_model_score_svm = model_score
        best_model_svm = clf_svm
        print('eta0:',eta)
        print('best val acc:',best_model_score_svm)
                                        
# print("best model score:" ,best_model_score_svm)
# print(classification_report(val_y['target'], best_model_svm.predict(val_X)))
# print(roc_auc_score(val_y['target'], best_model_svm.predict(val_X)>0.5))
#print(roc_auc_score(val_y['target'], clf_svm.predict_proba(val_X)[:,1]))

print("best model score:" ,best_model_score_svm)
print(classification_report(val_y['target'], best_model_svm.predict(val_X)))
print(roc_auc_score(val_y['target'], best_model_svm.predict(val_X)>0.5))

"""best logistic regression model(with no penalty):  
learning rate = 'adaptive', eta = 0.01  
best val acc = 0.694
"""

#fine tune learning rate on logistic regression
best_model_df = None
best_model_score_df = 0.0
learning_rate = ['optimal','invscaling','adaptive']
etas = [0.01,0.001,0.0001]
for lr in learning_rate:
    for eta in etas:
        clf_log_df = Pipeline(steps=[('preprocessor', preprocessor),
                        ('classifier', SGDClassifier(loss='log', penalty=None, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, 
                                        max_iter=1000, tol=0.001, shuffle=True, verbose=0, epsilon=0.1, n_jobs=None, 
                                        random_state=None, learning_rate=lr, eta0=eta, 
                                        power_t=0.5, early_stopping=True, validation_fraction=0.1, n_iter_no_change=5, 
                                        class_weight=None, warm_start=True, average=False))])
        clf_log_df.fit(train_X, train_y['target'])
        model_score = clf_log_df.score(val_X, val_y['target'])
        if model_score > best_model_score_df:
            best_model_score_df = model_score
            best_model_df = clf_log_df
            print('learning_rate:',lr,'eta0:',eta)
            print('best val acc:',best_model_score_df)
                                        
print("best model score:" ,best_model_score_df)
print(classification_report(val_y['target'], best_model_df.predict(val_X)))
print(roc_auc_score(val_y['target'], best_model_df.predict_proba(val_X)[:,1]))

#fine tune SVM and Logistic regression with penalty
best_model = None
best_model_score = 0.0
loss = ['log', 'hinge']
penalty = ['l1','l2','elasticnet']

for ls in loss:
    for plty in penalty: 
        clf_log_tune = Pipeline(steps=[('preprocessor', preprocessor),
                        ('classifier', SGDClassifier(loss=ls, penalty=plty, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, 
                                        max_iter=1000, tol=0.001, shuffle=True, verbose=0, epsilon=0.1, n_jobs=None, 
                                        random_state=None, learning_rate='adaptive', eta0=0.01, 
                                        power_t=0.5, early_stopping=True, validation_fraction=0.1, n_iter_no_change=5, 
                                        class_weight=None, warm_start=True, average=False))])
        clf_log_tune.fit(train_X, train_y['target'])
        model_score = clf_log_tune.score(val_X, val_y['target'])
        print( 'loss:', ls, 'penalty:', plty,'val acc:', model_score)
        if model_score > best_model_score:
            best_model_score = model_score
            best_model = clf_log_tune
            print( 'best loss:', ls, 'best penalty:', plty,'best val acc:', model_score)

print("best model score:" ,best_model_score)
print(classification_report(val_y['target'], best_model.predict(val_X)))
print(roc_auc_score(val_y['target'], best_model.predict_proba(val_X)[:,1]))

"""# SVM with kernel"""

#rbf
from sklearn.kernel_approximation import RBFSampler
numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])
categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])
preprocessor = ColumnTransformer(transformers=[('num', numeric_transformer, numCols),
                                               ('cat', categorical_transformer, catCols)])
rbf_feature = RBFSampler(gamma=0.2, random_state=1)
clf_kernel = Pipeline(steps=[('preprocessor', preprocessor),
                ('kernel',rbf_feature),
                ('classifier', SGDClassifier(loss='hinge', penalty=None, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, 
                                max_iter=1000, tol=0.001, shuffle=True, verbose=1, epsilon=0.1, n_jobs=None, 
                                random_state=None, learning_rate='adaptive', eta0=0.01, 
                                power_t=0.5, early_stopping=True, validation_fraction=0.1, n_iter_no_change=5))])
#default, no penalty
clf_kernel.fit(train_tiny_X, train_tiny_y['target'])
print("model score: %.3f" % clf_kernel.score(val_tiny_X, val_tiny_y['target']))
print(classification_report(val_tiny_y['target'], clf_kernel.predict(val_tiny_X)))
#print(roc_auc_score(val_tiny_y['target'], clf_kernel.predict_proba(val_tiny_X)[:,1]))

"""## Logistic Regression with Regularization

### L1
"""

from sklearn.feature_selection import SelectKBest
numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])
categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])
preprocessor = ColumnTransformer(transformers=[('num', numeric_transformer, numCols),
                                               ('cat', categorical_transformer, catCols)])
clf = Pipeline(steps=[('preprocessor', preprocessor),
                      ('classifier', LogisticRegression(penalty='l1'))])

clf.fit(train_tiny_X, train_tiny_y['target'])
print("model score: %.3f" % clf.score(val_tiny_X, val_tiny_y['target']))
print(classification_report(train_tiny_y['target'], clf.predict(train_tiny_X)))
print(roc_auc_score(train_tiny_y['target'], clf.predict_proba(train_tiny_X)[:,1]))

print("model score: %.3f" % clf.score(val_tiny_X, val_tiny_y['target']))
print(classification_report(val_tiny_y['target'], clf.predict(val_tiny_X)))
print(roc_auc_score(val_tiny_y['target'], clf.predict_proba(val_tiny_X)[:,1]))

"""## L2"""

clf = Pipeline(steps=[('preprocessor', preprocessor),
                      ('classifier', LogisticRegression(penalty='l2'))])

clf.fit(train_tiny_X, train_tiny_y['target'])
print("model score: %.3f" % clf.score(val_tiny_X, val_tiny_y['target']))
print(classification_report(train_tiny_y['target'], clf.predict(train_tiny_X)))
print(roc_auc_score(train_tiny_y['target'], clf.predict_proba(train_tiny_X)[:,1]))

print("model score: %.3f" % clf.score(val_tiny_X, val_tiny_y['target']))
print(classification_report(val_tiny_y['target'], clf.predict(val_tiny_X)))
print(roc_auc_score(val_tiny_y['target'], clf.predict_proba(val_tiny_X)[:,1]))

"""## Gradient Boosting & Decision Tree
Moved to "Tree & Boosting" Notebook

## Lasso
"""

from sklearn.linear_model import Lasso
alphas = [0.01,0.05,0.1,0.2]
for a in alphas:
    print('alpha: {}'.format(a))
    lasso = Pipeline(steps=[('preprocessor', preprocessor),
                      ('regressor',  Lasso(alpha = a))])
    lasso.fit(train_tiny_X, train_tiny_y['target'])
    print(np.sqrt(metrics.mean_squared_error(val_tiny_y['target'], lasso.predict(val_tiny_X))))

gc.collect()

